# Infra
# import cv2
# import numpy as np
# from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim
# import torch
# import lpips
# from torchvision import transforms, models
# from scipy import linalg
# import os

# # æ£€æŸ¥torchvisionç‰ˆæœ¬å¹¶é€‰æ‹©åˆé€‚çš„å¯¼å…¥æ–¹å¼
# try:
#     from torchvision.models.video import R3D_18_Weights
#     HAS_NEW_TORCHVISION = True
# except ImportError:
#     HAS_NEW_TORCHVISION = False
#     print("è­¦å‘Š: ä½¿ç”¨æ—§ç‰ˆtorchvision API")

# # -------------------------------
# # 1. è¯»å–è§†é¢‘å¸§
# # -------------------------------
# def read_video_frames(video_path, to_rgb=True, resize=None):
#     """å®‰å…¨åœ°è¯»å–è§†é¢‘å¸§ï¼Œå¤„ç†å„ç§é”™è¯¯æƒ…å†µ"""
#     if not os.path.exists(video_path):
#         print(f"é”™è¯¯: è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
#         return []
    
#     cap = cv2.VideoCapture(video_path)
#     if not cap.isOpened():
#         print(f"é”™è¯¯: æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
#         return []
    
#     frames = []
#     frame_count = 0
#     max_frames = 100  # é™åˆ¶æœ€å¤§å¸§æ•°ï¼Œé¿å…å†…å­˜é—®é¢˜
    
#     while True:
#         ret, frame = cap.read()
#         if not ret:
#             break
            
#         if to_rgb:
#             # ç¡®ä¿æ˜¯3é€šé“å›¾åƒ
#             if len(frame.shape) == 2:  # ç°åº¦å›¾
#                 frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)
#             elif frame.shape[2] == 3:  # BGR
#                 frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#             elif frame.shape[2] == 4:  # BGRA
#                 frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2RGB)
        
#         if resize is not None:
#             frame = cv2.resize(frame, resize)
        
#         frames.append(frame)
#         frame_count += 1
        
#         if frame_count >= max_frames:
#             break
    
#     cap.release()
    
#     if len(frames) == 0:
#         print(f"è­¦å‘Š: è§†é¢‘ {video_path} æ²¡æœ‰è¯»å–åˆ°ä»»ä½•å¸§")
    
#     print(f"ä» {video_path} è¯»å–äº† {len(frames)} å¸§")
#     return frames


# # -------------------------------
# # 2. è®¡ç®— PSNR å’Œ SSIM
# # -------------------------------
# def compute_psnr_ssim(gt_frames, pre_frames):
#     if len(gt_frames) == 0 or len(pre_frames) == 0:
#         print("é”™è¯¯: æ— æ³•è®¡ç®—PSNR/SSIMï¼Œå¸§åˆ—è¡¨ä¸ºç©º")
#         return 0, 0
    
#     psnr_list, ssim_list = [], []
#     min_len = min(len(gt_frames), len(pre_frames))
    
#     for i in range(min_len):
#         gt = gt_frames[i].astype(np.float32)
#         pre = pre_frames[i].astype(np.float32)
        
#         # ç¡®ä¿å›¾åƒå°ºå¯¸ç›¸åŒ
#         if gt.shape != pre.shape:
#             pre = cv2.resize(pre, (gt.shape[1], gt.shape[0]))
        
#         try:
#             psnr_val = psnr(gt, pre, data_range=255)
#             # SSIMéœ€è¦å¤„ç†å¤šé€šé“
#             if len(gt.shape) == 3 and gt.shape[2] == 3:
#                 ssim_val = ssim(gt, pre, data_range=255, channel_axis=2, win_size=7)
#             else:
#                 ssim_val = ssim(gt, pre, data_range=255, win_size=7)
            
#             psnr_list.append(psnr_val)
#             ssim_list.append(ssim_val)
#         except Exception as e:
#             print(f"è®¡ç®—PSNR/SSIMæ—¶å‡ºé”™: {e}")
#             continue
    
#     if len(psnr_list) == 0:
#         return 0, 0
    
#     return np.mean(psnr_list), np.mean(ssim_list)


# # -------------------------------
# # 3. è®¡ç®— LPIPS
# # -------------------------------
# def compute_lpips(gt_frames, pre_frames, device='cpu'):
#     if len(gt_frames) == 0 or len(pre_frames) == 0:
#         print("é”™è¯¯: æ— æ³•è®¡ç®—LPIPSï¼Œå¸§åˆ—è¡¨ä¸ºç©º")
#         return 1.0  # æœ€å·®æƒ…å†µ
    
#     try:
#         loss_fn = lpips.LPIPS(net='alex').to(device)
#         to_tensor = transforms.ToTensor()
#         lpips_scores = []
#         min_len = min(len(gt_frames), len(pre_frames))
        
#         for i in range(min_len):
#             gt = gt_frames[i]
#             pre = pre_frames[i]
            
#             # ç¡®ä¿å›¾åƒå°ºå¯¸ç›¸åŒ
#             if gt.shape != pre.shape:
#                 pre = cv2.resize(pre, (gt.shape[1], gt.shape[0]))
            
#             # ç¡®ä¿æ˜¯3é€šé“
#             if len(gt.shape) == 2:
#                 gt = cv2.cvtColor(gt, cv2.COLOR_GRAY2RGB)
#             if len(pre.shape) == 2:
#                 pre = cv2.cvtColor(pre, cv2.COLOR_GRAY2RGB)
            
#             gt_t = to_tensor(gt).unsqueeze(0).to(device) * 2 - 1
#             pre_t = to_tensor(pre).unsqueeze(0).to(device) * 2 - 1
            
#             with torch.no_grad():
#                 d = loss_fn(gt_t, pre_t)
#             lpips_scores.append(d.item())
        
#         return np.mean(lpips_scores) if lpips_scores else 1.0
#     except Exception as e:
#         print(f"è®¡ç®—LPIPSæ—¶å‡ºé”™: {e}")
#         return 1.0


# # -------------------------------
# # 4. è®¡ç®— FVDï¼ˆç®€åŒ–ç‰ˆï¼Œç”¨ R3D-18 ç‰¹å¾ï¼‰
# # -------------------------------
# def extract_video_feature(frames, device='cpu'):
#     """æå–è§†é¢‘ç‰¹å¾ï¼Œå¤„ç†ç©ºå¸§æƒ…å†µ"""
#     if len(frames) == 0:
#         print("é”™è¯¯: æ— æ³•æå–è§†é¢‘ç‰¹å¾ï¼Œå¸§åˆ—è¡¨ä¸ºç©º")
#         return np.zeros(512)  # è¿”å›é›¶å‘é‡
    
#     try:
#         # æ ¹æ®torchvisionç‰ˆæœ¬é€‰æ‹©åˆé€‚çš„API
#         if HAS_NEW_TORCHVISION:
#             weights = R3D_18_Weights.DEFAULT
#             model = models.video.r3d_18(weights=weights).to(device)
#         else:
#             # ä½¿ç”¨æ—§ç‰ˆAPI
#             model = models.video.r3d_18(pretrained=True).to(device)
        
#         model.eval()

#         transform = transforms.Compose([
#             transforms.ToPILImage(),
#             transforms.Resize((112, 112)),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], 
#                                std=[0.22803, 0.22145, 0.216989])
#         ])

#         # é€‰æ‹©å‰16å¸§æˆ–æ‰€æœ‰å¸§ï¼ˆå¦‚æœå°‘äº16å¸§ï¼‰
#         num_frames = min(16, len(frames))
#         selected_frames = frames[:num_frames]
        
#         tensors = []
#         for frame in selected_frames:
#             # ç¡®ä¿æ˜¯3é€šé“
#             if len(frame.shape) == 2:
#                 frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)
#             tensor = transform(frame)
#             tensors.append(tensor)
        
#         if not tensors:
#             return np.zeros(512)
            
#         # å †å å¼ é‡ (C, T, H, W)
#         video_tensor = torch.stack(tensors, dim=1).unsqueeze(0).to(device)

#         with torch.no_grad():
#             # ä½¿ç”¨R3D-18çš„å‰å‘ä¼ æ’­
#             features = model(video_tensor)
#             feat = features.mean(dim=[2, 3, 4]) if features.dim() > 2 else features
        
#         return feat.cpu().numpy().flatten()
        
#     except Exception as e:
#         print(f"æå–è§†é¢‘ç‰¹å¾æ—¶å‡ºé”™: {e}")
#         return np.zeros(512)


# def frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):
#     """è®¡ç®—Frechetè·ç¦»"""
#     diff = mu1 - mu2
    
#     # é˜²æ­¢å¥‡å¼‚çŸ©é˜µ
#     sigma1 = sigma1 + eps * np.eye(sigma1.shape[0])
#     sigma2 = sigma2 + eps * np.eye(sigma2.shape[0])
    
#     covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
#     if np.iscomplexobj(covmean):
#         covmean = covmean.real
    
#     return diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)


# def compute_fvd(gt_frames, pre_frames, device='cpu'):
#     """è®¡ç®—FVDï¼Œå¤„ç†å„ç§é”™è¯¯æƒ…å†µ"""
#     if len(gt_frames) == 0 or len(pre_frames) == 0:
#         print("é”™è¯¯: æ— æ³•è®¡ç®—FVDï¼Œå¸§åˆ—è¡¨ä¸ºç©º")
#         return float('inf')
    
#     try:
#         feat1 = extract_video_feature(gt_frames, device)
#         feat2 = extract_video_feature(pre_frames, device)
        
#         # è®¡ç®—å‡å€¼å’Œåæ–¹å·®
#         mu1, mu2 = np.mean(feat1), np.mean(feat2)
        
#         # å¯¹äºå•ä¸ªæ ·æœ¬ï¼Œä½¿ç”¨æ–¹å·®ä½œä¸ºåæ–¹å·®çŸ©é˜µçš„è¿‘ä¼¼
#         sigma1 = np.cov(feat1) if len(feat1) > 1 else np.var(feat1) * np.eye(len(feat1))
#         sigma2 = np.cov(feat2) if len(feat2) > 1 else np.var(feat2) * np.eye(len(feat2))
        
#         # ç¡®ä¿æ˜¯2Dæ•°ç»„
#         if sigma1.ndim == 0:
#             sigma1 = np.array([[sigma1]])
#         if sigma2.ndim == 0:
#             sigma2 = np.array([[sigma2]])
        
#         return frechet_distance(np.array([mu1]), sigma1, np.array([mu2]), sigma2)
        
#     except Exception as e:
#         print(f"è®¡ç®—FVDæ—¶å‡ºé”™: {e}")
#         return float('inf')


# # -------------------------------
# # 5. ä¸»å‡½æ•°
# # -------------------------------
# if __name__ == "__main__":
#     # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿æ–‡ä»¶å¯è®¿é—®
#     gt_path = "/data/zhounan/DiffSynth-Studio/data/my_fire_mask/Infrared_832_480_17/video_007.mp4"
#     pre_path = "/data/zhounan/DiffSynth-Studio/My_Results/video_Wan2.1-VACE-1.3B_Type1_e86_007-consistency.mp4"
#     # pre_path = "data/my_fire_mask/Infrared_832_480_17/video_006.mp4"
#     device = "cuda" if torch.cuda.is_available() else "cpu"

#     print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")

#     # è¯»å–è§†é¢‘å¸§
#     print("æ­£åœ¨è¯»å–çœŸå®è§†é¢‘...")
#     gt_frames = read_video_frames(gt_path)  # è°ƒæ•´å°ºå¯¸ä»¥èŠ‚çœå†…å­˜

#     print("æ­£åœ¨è¯»å–é¢„æµ‹è§†é¢‘...")
#     pre_frames = read_video_frames(pre_path)
#     pre_frames = pre_frames[16:33]
#     # pre_frames = pre_frames[0:16]

#     # æ£€æŸ¥æ˜¯å¦æˆåŠŸè¯»å–
#     if len(gt_frames) == 0 or len(pre_frames) == 0:
#         print("âŒ é”™è¯¯: æ— æ³•è¯»å–è§†é¢‘æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶æ ¼å¼")
#         exit(1)

#     # å¯¹é½å¸§æ•°ä¸å°ºå¯¸
#     min_len = min(len(gt_frames), len(pre_frames))
#     gt_frames = gt_frames[:min_len]
#     pre_frames = pre_frames[:min_len]

#     print(f"è§†é¢‘å¸§æ•°: {min_len}")

#     # ---- è®¡ç®—å„æŒ‡æ ‡ ----
#     print("è®¡ç®—PSNRå’ŒSSIM...")
#     psnr_val, ssim_val = compute_psnr_ssim(gt_frames, pre_frames)
    
#     print("è®¡ç®—LPIPS...")
#     lpips_val = compute_lpips(gt_frames, pre_frames, device=device)
    
#     print("è®¡ç®—FVD...")
#     fvd_val = compute_fvd(gt_frames, pre_frames, device=device)

#     print("\nğŸ“Š è®¡ç®—ç»“æœï¼š")
#     print(f"PSNR  = {psnr_val:.4f}")
#     print(f"SSIM  = {ssim_val:.4f}")
#     print(f"LPIPS = {lpips_val:.4f}")
#     print(f"FVD(approx) = {fvd_val:.4f}")

# Mask
import os
import cv2
import numpy as np
from sklearn.metrics import precision_recall_curve, average_precision_score, f1_score
import matplotlib.pyplot as plt
import torch

def load_video_as_mask(video_path, num_frames=17, height=480, width=832, threshold=0.9):
    """
    å°†è§†é¢‘è¯»ä¸º mask åºåˆ— (T, H, W), äºŒå€¼åŒ–å‡è®¾æ©ç é€šé“ä¸ºç°åº¦æˆ–å•é€šé“ã€‚
    å¦‚æœæ˜¯ RGB æ©ç ï¼Œéœ€è¦å…ˆè½¬ä¸ºç°åº¦æˆ–å–æŸä¸€é€šé“ã€‚
    threshold: ç°åº¦>threshold æ—¶åˆ¤ä¸º 1.
    """
    cap = cv2.VideoCapture(video_path)
    frames = []
    for _ in range(num_frames):
        ret, frame = cap.read()
        if not ret:
            raise RuntimeError(f"Cannot read frame from {video_path}")
        # å‡è®¾æ©ç æ˜¯ BGR ç°åº¦å åŠ æˆ–å½©è‰²ï¼Œä½†æ©ç ç”¨ç™½è‰²=1ï¼Œé»‘è‰²=0
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # å½’ä¸€åŒ–
        norm = gray.astype('float32') / 255.0
        binary = (norm >= threshold).astype(np.uint8)
        frames.append(binary)
    cap.release()
    arr = np.stack(frames, axis=0)  # (T, H, W)
    return arr

def compute_metrics(true_mask, pred_mask):
    """
    true_mask, pred_mask: shape (T, H, W), 0/1 values
    Returns: dict of AUPRC, F1, IoU, MSE
    """
    T, H, W = true_mask.shape
    # Flatten all pixels across time
    y_true = true_mask.reshape(-1)
    y_pred = pred_mask.reshape(-1)
    # AUPRC: need scores, butè¿™é‡Œç”¨ pred_mask (0/1)ä½œä¸ºåˆ†æ•°è¿‘ä¼¼
    # å¦‚æœä½ çš„é¢„æµ‹æ˜¯æ¦‚ç‡ï¼Œåˆ™ç”¨æ¦‚ç‡ã€‚è¿™é‡Œç”¨äºŒå€¼ç»“æœä¸€æ ·å¯å¾— precision_recall_curve
    ap = average_precision_score(y_true, y_pred)
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)
    # æ‰¾åˆ°æœ€ä½³ F1 across thresholds
    f1s = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_idx = np.nanargmax(f1s)
    best_f1 = f1s[best_idx]
    # IoU
    intersection = np.logical_and(y_true==1, y_pred==1).sum()
    union = np.logical_or(y_true==1, y_pred==1).sum()
    iou = intersection / union if union > 0 else 0.0
    # MSE (æŒ‰åƒç´ å€¼ 0/1)
    mse = np.mean((y_true.astype('float32') - y_pred.astype('float32'))**2)
    return {"AUPRC": ap, "BestF1": best_f1, "IoU": iou, "MSE": mse}

def visualize_masks(true_mask, pred_mask, output_dir=None, max_frames=5):
    """
    true_mask, pred_mask: numpy arrays of shape (T, H, W)
    è¾“å‡ºå‰ max_frames å¸§çš„å¯è§†åŒ–å¯¹æ¯”ï¼š
      - å·¦ä¾§ï¼šçœŸå€¼æ©ç 
      - å³ä¾§ï¼šé¢„æµ‹æ©ç 
    å¦‚æœæŒ‡å®š output_dirï¼Œä¼šå°†å›¾ä¿å­˜ä¸º .jpg æ–‡ä»¶ã€‚
    """
    T, H, W = true_mask.shape
    os.makedirs(output_dir, exist_ok=True) if output_dir else None
    
    for i in range(min(T, max_frames)):
        plt.figure(figsize=(8,4))
        plt.suptitle(f"Frame {i}")
        
        plt.subplot(1,2,1)
        plt.title("True mask")
        plt.imshow(true_mask[i], cmap='gray')
        plt.axis('off')
        
        plt.subplot(1,2,2)
        plt.title("Pred mask")
        plt.imshow(pred_mask[i], cmap='gray')
        plt.axis('off')
        
        plt.tight_layout()
        plt.show()
        
        if output_dir:
            save_path = os.path.join(output_dir, f"mask_compare_frame_{i:02d}.jpg")
            plt.savefig(save_path)
            plt.close()

if __name__ == "__main__":
    pred_path = "/data/zhounan/DiffSynth-Studio/My_Results/video_Wan2.1-VACE-1.3B_Multi-Setting1_e86_007-consistency.mp4"
    true_path = "/data/zhounan/DiffSynth-Studio/data/5_Regions/Lida_video/Mask_832_480_17/video_007.mp4"
    # num_frames = 17
    height = 480
    width = 832

    true_mask = load_video_as_mask(true_path, num_frames=16, height=height, width=width, threshold=0.5)
    pred_mask = load_video_as_mask(pred_path, num_frames=33, height=height, width=width, threshold=0.9)
    pred_mask = pred_mask[17:33]
    # pred_mask = pred_mask[:17]

    # visualize_masks(true_mask[-5:], pred_mask[-5:], output_dir="mask_vis", max_frames=16)

    metrics = compute_metrics(true_mask, pred_mask)
    print("Metrics:", metrics)